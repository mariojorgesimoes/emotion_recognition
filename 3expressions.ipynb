{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_8S5iydGcZFP"
      },
      "source": [
        "# Facial Expression Recognition with TensorFlow 2.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3B9Q9litc6dO"
      },
      "source": [
        "Mudar a versão do TensorFlow para 2.0"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3u82BeElcl7p"
      },
      "source": [
        " %tensorflow_version 2.x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sg8UkVbcdOEk"
      },
      "source": [
        "Habilitar GPU\n",
        "\n",
        "\n",
        "> Ir a Runtime no menu no topo --> Change runtime type --> Em Hardware accelerator, selecionar GPU."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "29vxjnW6c4ic",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c3bd13e3-bc27-4a6b-ecb7-21e8b0ac393a"
      },
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)\n",
        "#tf.debugging.set_log_device_placement(True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.7.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KwxtMjTjdL_7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9a15961a-c7a0-41d6-eb66-eca79606e65f"
      },
      "source": [
        "# check for GPU, at least 1 GPU needed\n",
        "print(tf.test.gpu_device_name())\n",
        "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/device:GPU:0\n",
            "Num GPUs Available:  1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rfkTxNLFk6OT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6641c293-4f1c-4c8d-e298-6ade74008ee7"
      },
      "source": [
        "# Check GPU information\n",
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sun Dec  5 15:09:20 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 495.44       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   50C    P0    68W / 149W |    144MiB / 11441MiB |      1%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "84QJGIXg6CO0"
      },
      "source": [
        "# Fazer import dos packages necessários"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u8vgxRuH6X9B"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import cv2\n",
        "from math import sin, cos, pi\n",
        "\n",
        "from keras.layers import Conv2D, LeakyReLU, GlobalAveragePooling2D, Dropout, Dense\n",
        "from keras.models import Sequential"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O0o8IVaX6lo6"
      },
      "source": [
        "def plot_sample(image, keypoint, axis, title):\n",
        "    image = image.reshape(96,96)\n",
        "    axis.imshow(image, cmap='gray')\n",
        "    axis.scatter(keypoint[0::2], keypoint[1::2], marker='x', s=20)\n",
        "    plt.title(title)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BBS7WA1AeiwT"
      },
      "source": [
        "## Dataset (import, preparation, preprocessing)\n",
        "\n",
        "> Banco de dados do desafio Kaggle, mais informações em:\n",
        "https://www.kaggle.com/c/challenges-in-representation-learning-facial-expression-recognition-challenge\n",
        "\n",
        "O banco de dados possui 3 colunas: emotions | pixels | Usage\n",
        "\n",
        "**Emotions**: são as emoções das imagens: 0=Angry, 1=Disgust, 2=Fear, 3=Happy, 4=Sad, 5=Surprise, 6=Neutral\n",
        "\n",
        "**Pixels**: é a imagem 64x64 (matriz de pixels )\n",
        "\n",
        "**Usage**: indica se é para ser usado como trainamento ou teste\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M82riQbAW3UY"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N9wNfb99e01_"
      },
      "source": [
        "# Download dataset fer2013\n",
        "! wget https://github.com/offsouza/Facial-Expression-Recognition/raw/master/fer2013.csv.zip"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TQC0p7SPjQLB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6f8e20ee-cf9b-4227-b654-5d5b6b7b5cd2"
      },
      "source": [
        "#Unzip file .zip\n",
        "! unzip https://drive.google.com/file/d/1fjrs2UzDPBuPggxv1JTpYd7mush73tpw/view?usp=sharing"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "unzip:  cannot find or open https://drive.google.com/file/d/1fjrs2UzDPBuPggxv1JTpYd7mush73tpw/view?usp=sharing, https://drive.google.com/file/d/1fjrs2UzDPBuPggxv1JTpYd7mush73tpw/view?usp=sharing.zip or https://drive.google.com/file/d/1fjrs2UzDPBuPggxv1JTpYd7mush73tpw/view?usp=sharing.ZIP.\n",
            "\n",
            "No zipfiles found.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VicWnaeKn89r"
      },
      "source": [
        "Importar banco de dados"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PZPU7Agknbrt",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 415
        },
        "outputId": "0a0691b0-8d8a-4216-ce22-cf64c4b9a49e"
      },
      "source": [
        "import pandas as pd\n",
        "data = pd.read_csv('fer2013.csv')\n",
        "data.head()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-9c383ff25678>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'fer2013.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    686\u001b[0m     )\n\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 688\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    689\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 454\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    946\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    947\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 948\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    950\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1178\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1180\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1181\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   2008\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2009\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2010\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2011\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2012\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'fer2013.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mTnfZLIp1pAG"
      },
      "source": [
        "Verificar a quantidade de dados\n",
        "\n",
        "Existe 35887 imagens com os  seguintes labels [0 1 2 3 5 6 ]\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "87iTYlnbStRx"
      },
      "source": [
        "print('Count: ', data.count())\n",
        "print('Emotions labels', data.emotion.unique())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nkxg59_X1xOG"
      },
      "source": [
        "Verificar a quantidade de imagens de cada emoção"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uSh3NaKT1zwh"
      },
      "source": [
        "label_map =  { 0: \"Angry\", 1:'Disgust', 2:'Fear', 3:'Happy', 4:\"Sad\", 5:'Surprise', 6:'Neutral'}\n",
        "\n",
        "df = data['emotion'].value_counts()\n",
        "df = pd.DataFrame(df)\n",
        "df = df.rename(index=label_map)\n",
        "df\n",
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gXGSRfjhpwnD"
      },
      "source": [
        "Nesse código não se treina todas as emoções, identificamos somente 4 emoções:\n",
        "\n",
        "> 3=Happy, 4=Sad, 5=Surprise, 6=Neutral\n",
        "\n",
        "Para isso iremos ter que remover todos os dados com os Labels 0,1 e 2\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4oQrxA6Ipu-r"
      },
      "source": [
        "# Remove as expressões Angry=0, Disgust=1, fear = 2\n",
        "data  = data[data['emotion'] != 0 ]\n",
        "data  = data[data['emotion'] != 1 ]\n",
        "data  = data[data['emotion'] != 2 ]\n",
        "print('Count: ', data.count())\n",
        "print('Emotions labels', data.emotion.unique() )\n",
        "# aqui é realizado a substituição do labels\n",
        "# Isso é feito pois, se deixar os labels de 3 a 6 irá ser gerado um vetor de 7 (0 a 6) posições na última camada, assim o modelo irá\n",
        "# entender que existe 7 tipos de emoções. Assim, transformando os labels de 0 a 3, gerando vetor de 4 posições somente.\n",
        "data['emotion'] = data['emotion'].replace(5, 1)\n",
        "data['emotion'] = data['emotion'].replace(6, 0)\n",
        "data['emotion'] = data['emotion'].replace(4, 2)\n",
        "\n",
        "#label_map_new\n",
        "# 0: Neutro, 1:Suprise, 2: Triste, 3: Happy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1oUIJ1WC4WJ1"
      },
      "source": [
        "Converter a coluna 'pixels' de String para Texto"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LF6XcgosadaJ"
      },
      "source": [
        "# carregar imagens\n",
        "import numpy as np\n",
        "\n",
        "pix = data['pixels']  # Os pixels estão como texto (string) e não valor numérico, então passa-se o dataframe para um lista\n",
        "pix = list(pix) # converter para lista\n",
        "print(\"pix lista, primeiro valor: \\n\", pix[0:1],'\\n\\n') # imprimir a lista no formato formato texto\n",
        "lista_pix_int = list() # criar uma lista vazia\n",
        "\n",
        "from tqdm import tqdm   # mostrar a barra de progresso\n",
        "for i in tqdm(pix):\n",
        "    # aqui eu faço um split da lista para que após o numero tenha uma virgula assim mudando de ['24 32 36' ] para ['24', '32', '36']\n",
        "    # no mesmo comando transforma-se de string(ex:'24') para inteiro(ex:24) assim obtendo um vetor(lista) de numero inteiros e não mais\n",
        "    # de strings\n",
        "    pix_int = [float(j) for j in i.split() ]\n",
        "    lista_pix_int.append(pix_int) # realizar a transformação num dos vetores, adiciona-se a uma lista\n",
        "\n",
        "\n",
        "array_pix = np.array(lista_pix_int) # Converter a lista com valores numéricos para um array numpy\n",
        "print(\"pix array, primeiro valor: \\n\", array_pix[0:1],'\\n\\n') # verifica-se que a lista está em formato float\n",
        "print()\n",
        "print('---\\nShape: ', array_pix.shape)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3hm11eh5nTls"
      },
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "scaler = MinMaxScaler()\n",
        "df_n = scaler.fit_transform(array_pix)\n",
        "\n",
        "print(df_n)\n",
        "\n",
        "array_pix = df_n\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qcLe5f4I0aVv"
      },
      "source": [
        "df_n[0:1]\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HwpgynnkV-iH"
      },
      "source": [
        "  # observando o shape (22366, 2304)\n",
        "  # temos 22366 imagens que estão em um veteor de 2304 posições\n",
        "  # porém sabemos que as imagens tem tamanho 48x48 = 2304\n",
        "\n",
        "  #  Então temos que redimensionars todas as imagens para 48 x 48\n",
        "\n",
        "print(type(array_pix))\n",
        "reshape_pix = array_pix.reshape(array_pix.shape[0], 48,48,1)\n",
        "print(\"Shape antigo:\" , array_pix.shape)\n",
        "print(\"Shape novo:\" , reshape_pix.shape)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R40Jz0pcuHsP"
      },
      "source": [
        "# agora temos nossa imagem ou seja nossos dados de entrada no formato adequado\n",
        "# Temos que dividir nossos dados em dados de treino, teste e validação\n",
        "# Irei dividir dados de treino em 60% , Teste 20% e Validação 20%\n",
        "from keras.utils import np_utils\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X = reshape_pix\n",
        "y = data['emotion'].values\n",
        "\n",
        "\n",
        "labels = pd.DataFrame(y)\n",
        "y = np_utils.to_categorical(labels)\n",
        "\n",
        "X_train, X_test_val, y_train, y_test_val = train_test_split(X, y, test_size=0.4, random_state=42,shuffle= True)\n",
        "\n",
        "print('X_train: ', X_train.shape)\n",
        "print('y_train: ', y_train.shape)\n",
        "print('X_teste_val: ', X_test_val.shape)\n",
        "print('y_teste_val: ', y_test_val.shape)\n",
        "\n",
        "X_test, X_val, y_test, y_val = train_test_split(X_test_val, y_test_val, test_size=0.5, random_state=42, shuffle= True)\n",
        "print('\\n ---------------\\n')\n",
        "print('X_teste: ', X_test.shape)\n",
        "print('y_test: ', y_test.shape)\n",
        "print('X_val: ', X_val.shape)\n",
        "print('y_val: ', y_val.shape)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oLTCd2aE7dBP"
      },
      "source": [
        "y[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M4ra5Vo0xTQJ"
      },
      "source": [
        "# Aqui faço o reshape novamente somente para exibir as imagens\n",
        "x = X_train.reshape(X_train.shape[0], 48,48)\n",
        "\n",
        "# plot\n",
        "import matplotlib.pyplot as plt\n",
        "j =0\n",
        "for i in range(8, 14):\n",
        "    j+=1\n",
        "    plt.subplot(2,3 ,j)\n",
        "    image = x[i]\n",
        "    plt.imshow(image, cmap= 'gray')\n",
        "    #plt.text(0,45, label_map[y_train[i]], fontsize = 20, color = 'red')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Epp-_d1fluch"
      },
      "source": [
        "# Criação do Modelo CNN\n",
        "\n",
        "A rede neural que será construida terá como base a arquitetura VGG16"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YP8qR9ZSzIoH"
      },
      "source": [
        "# importando todos os modulos\n",
        "\n",
        "import  os\n",
        "import  tensorflow as tf\n",
        "from    tensorflow import keras\n",
        "from    tensorflow.keras import datasets, layers, optimizers, models\n",
        "from    tensorflow.keras import regularizers\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2EG-CnFmu8Hx"
      },
      "source": [
        "class Block_ConvReluBN (keras.Model):\n",
        "\n",
        "  def __init__(self, filters, strides=1, kernel=3, padding = 'same', dp = 0.3):\n",
        "    super(Block_ConvReluBN, self).__init__()\n",
        "\n",
        "    self.conv =  keras.layers.Conv2D(filters, kernel, strides=strides, padding=padding)\n",
        "    self.bn= keras.layers.BatchNormalization()\n",
        "    self.relu = keras.layers.Activation('relu')\n",
        "    self.drop = keras.layers.Dropout(dp)\n",
        "\n",
        "  def call(self,x,training=None):\n",
        "\n",
        "    x = self.conv(x )\n",
        "    x = self.bn(x, training = training)\n",
        "    x = self.relu(x)\n",
        "    x = self.drop(x)\n",
        "\n",
        "    return x\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vfojusEiWBeP"
      },
      "source": [
        "## Criando classe do modelo VGG16\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yHuYbDiPOVBi"
      },
      "source": [
        "class Model_VGG16(keras.Model):\n",
        "\n",
        "  def __init__(self,num_classes,**kwargs):\n",
        "    super(Model_VGG16, self).__init__(**kwargs)\n",
        "\n",
        "    self.num_classes = num_classes\n",
        "\n",
        "    self.block01 = Block_ConvReluBN(64)\n",
        "    self.block02 = Block_ConvReluBN(64)\n",
        "    self.maxpool1 =  keras.layers.MaxPooling2D(pool_size=(2, 2))\n",
        "\n",
        "    self.block03 = Block_ConvReluBN(128)\n",
        "    self.block04 = Block_ConvReluBN(128)\n",
        "    self.maxpool2 =  keras.layers.MaxPooling2D(pool_size=(2, 2))\n",
        "\n",
        "    self.block05 = Block_ConvReluBN(256)\n",
        "    self.block06 = Block_ConvReluBN(256)\n",
        "    self.block07 = Block_ConvReluBN(256)\n",
        "    self.maxpool3 =  keras.layers.MaxPooling2D(pool_size=(2, 2))\n",
        "\n",
        "    self.block08 = Block_ConvReluBN(512)\n",
        "    self.block09 = Block_ConvReluBN(512)\n",
        "    self.block10 = Block_ConvReluBN(512)\n",
        "    self.maxpool4 =  keras.layers.MaxPooling2D(pool_size=(2, 2))\n",
        "\n",
        "    self.block11 = Block_ConvReluBN(512)\n",
        "    self.block12 = Block_ConvReluBN(512)\n",
        "    self.block13 = Block_ConvReluBN(512)\n",
        "    self.maxpool5 =  keras.layers.MaxPooling2D(pool_size=(2, 2))\n",
        "\n",
        "    self.flatten = keras.layers.Flatten()\n",
        "    self.dense = keras.layers.Dense(512,kernel_regularizer=regularizers.l2(0.0001))\n",
        "    self.bn2 = keras.layers.BatchNormalization()\n",
        "    self.relu2 = keras.layers.Activation('relu')\n",
        "\n",
        "    self.out = keras.layers.Dense(self.num_classes)\n",
        "\n",
        "  def call(self, inputs,training=None):\n",
        "    x= inputs\n",
        "    x = self.block01(x)\n",
        "    x = self.block02(x)\n",
        "    x = self.maxpool1(x)\n",
        "\n",
        "    x = self.block03(x)\n",
        "    x = self.block04(x)\n",
        "    x = self.maxpool2(x)\n",
        "\n",
        "    x = self.block05(x)\n",
        "    x = self.block06(x)\n",
        "    x = self.block07(x)\n",
        "    x = self.maxpool3(x)\n",
        "\n",
        "    x = self.block08(x)\n",
        "    x = self.block09(x)\n",
        "    x = self.block10(x)\n",
        "    x = self.maxpool4(x)\n",
        "\n",
        "    x = self.block11(x)\n",
        "    x = self.block12(x)\n",
        "    x = self.block13(x)\n",
        "    x = self.maxpool5(x)\n",
        "\n",
        "    x = self.flatten(x)\n",
        "    x = self.dense(x)\n",
        "    x = self.relu2(x)\n",
        "    x = self.bn2(x, training)\n",
        "    x = self.out(x)\n",
        "\n",
        "    return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oN_PNMybTZ7M"
      },
      "source": [
        "Build do modelo\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bZ71y47aOVEe"
      },
      "source": [
        "batch_size = 32\n",
        "classes = 4\n",
        "\n",
        "# build model and optimizer\n",
        "model = Model_VGG16(num_classes=classes)\n",
        "\n",
        "\n",
        "model.build(input_shape=(None, 48, 48,1))\n",
        "\n",
        "\n",
        "print(\"Number of variables in the model :\", len(model.trainable_variables))\n",
        "\n",
        "model.summary()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aqlTLJS9TfbY"
      },
      "source": [
        "### Treinamento usando Keras"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u54666-0w2T1"
      },
      "source": [
        "# treinamento keras\n",
        "# compilando o otimizador, função de perda e metricas\n",
        "model.compile(optimizer=keras.optimizers.Adam(0.001),\n",
        "              loss=keras.losses.CategoricalCrossentropy(from_logits=True),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "check = ModelCheckpoint(filepath='weights_keras.hdf5', verbose=1, save_best_only=True)\n",
        "#early = EarlyStopping(monitor='val_loss',patience=10)\n",
        "\n",
        "# train\n",
        "history = model.fit(X_train, y_train, batch_size=100, epochs=20, verbose=1,\n",
        "         validation_data=(X_val , y_val)) # callbacks=[ check, early]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zm-pdXvT2Mr_"
      },
      "source": [
        "evaluate the model and compute loss and accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0eLBMQCWFO8r"
      },
      "source": [
        "# evaluate on test set\n",
        "#scores = model.evaluate(X_test, y_test, batch_size, verbose=1)\n",
        "#print(\"Final test loss and accuracy :\", scores)\n",
        "\n",
        "sns.set_style('darkgrid')\n",
        "\n",
        "fig, ax = plt.subplots(2, 1, figsize=(20, 10))\n",
        "df = pd.DataFrame(history.history)\n",
        "df[['loss', 'val_loss']].plot(ax=ax[0])\n",
        "df[['accuracy', 'val_accuracy']].plot(ax=ax[1])\n",
        "ax[0].set_title('Model Loss', fontsize=12)\n",
        "ax[1].set_title('Model Accuracy', fontsize=12)\n",
        "fig.suptitle('Model Metrics', fontsize=18);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pfg3ZsbaHnoY"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R6ZyH7jP4k-G"
      },
      "source": [
        "Save the data obtained from the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PrS2etdgO-e_"
      },
      "source": [
        "model.save('model_full2', save_format=\"tf\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dDv4ij3uYIjH"
      },
      "source": [
        "!zip -r model.zip model_full/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oeNGKF8VTm9P"
      },
      "source": [
        "### Treinamento Customizado com Gradient.tape())"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DN_G-M-3gEwU"
      },
      "source": [
        "batch_size = 32\n",
        "classes = 4\n",
        "\n",
        "# build model and optimizer\n",
        "model = Model_VGG16(num_classes=classes)\n",
        "\n",
        "\n",
        "model.build(input_shape=(None, 48, 48,1))\n",
        "\n",
        "\n",
        "print(\"Number of variables in the model :\", len(model.trainable_variables))\n",
        "\n",
        "model.summary()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nrPQJAyK147-"
      },
      "source": [
        "\n",
        "optimizer = keras.optimizers.Adam(learning_rate=0.001)\n",
        "criteon = keras.losses.CategoricalCrossentropy(from_logits=True)\n",
        "\n",
        "acc_meter_train = tf.keras.metrics.CategoricalAccuracy(name='acc_train')\n",
        "\n",
        "loss_metric_train = tf.keras.metrics.Mean(name='train_loss')\n",
        "\n",
        "acc_meter_test = tf.keras.metrics.CategoricalAccuracy(name='acc_test')\n",
        "\n",
        "loss_metric_test = tf.keras.metrics.Mean(name='test_loss')\n",
        "\n",
        "db_train = tf.data.Dataset.from_tensor_slices((X_train, y_train)).batch(100)\n",
        "db_test = tf.data.Dataset.from_tensor_slices((X_test, y_test)).batch(100)\n",
        "\n",
        "geral_loss = 1000\n",
        "for epoch in range(50):\n",
        "\n",
        "\n",
        "    for step, (x, y) in enumerate(db_train):\n",
        "\n",
        "        with tf.GradientTape() as tape:\n",
        "\n",
        "            logits = model(x, training=True)\n",
        "            reg_loss=tf.math.add_n(model.losses)\n",
        "\n",
        "            loss = criteon(y, logits)\n",
        "            loss = loss + reg_loss\n",
        "\n",
        "\n",
        "        grads = tape.gradient(loss, model.trainable_variables)\n",
        "        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "\n",
        "        loss_metric_train.update_state(loss)\n",
        "        acc_meter_train.update_state(tf.argmax(y, axis=1),tf.argmax(logits, axis=1))\n",
        "\n",
        "    print('------- EPOCA %s -------'%epoch)\n",
        "    print('Train Acc:', acc_meter_train.result().numpy())\n",
        "    print('Train Loss: ', loss_metric_train.result().numpy())\n",
        "\n",
        "\n",
        "    loss_metric_train.reset_states()\n",
        "    acc_meter_train.reset_states()\n",
        "\n",
        "    for x, y in db_test:\n",
        "\n",
        "        logits = model(x, training=False)\n",
        "        val_loss = criteon(y, logits)\n",
        "        pred = tf.argmax(logits, axis=1)\n",
        "\n",
        "        y = tf.argmax(y, axis=1)\n",
        "\n",
        "        loss_metric_test.update_state(val_loss)\n",
        "        acc_meter_test.update_state(y, pred)\n",
        "\n",
        "    print('Val acc:', acc_meter_test.result().numpy())\n",
        "    print('Val loss:', loss_metric_test.result().numpy())\n",
        "\n",
        "    val_loss = loss_metric_test.result().numpy()\n",
        "\n",
        "    if val_loss < geral_loss:\n",
        "      model.save_weights('modelo.hdf5')\n",
        "      geral_loss = val_loss\n",
        "      print(\"modelo salvo\")\n",
        "\n",
        "    loss_metric_test.reset_states()\n",
        "    acc_meter_test.reset_states()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C8E3Dg2RgDGW"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4jN9Wb-nUEWn"
      },
      "source": [
        "Tesntando modelo com datos de teste"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KpIGf9JSDkJ6"
      },
      "source": [
        "model.compile(optimizer=keras.optimizers.Adam(0.001),\n",
        "                  loss='categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "scores = model.evaluate(X_test, y_test, verbose=1)\n",
        "print(\"Final test loss and accuracy :\", scores)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1GORyy4tVP2D"
      },
      "source": [
        "Função usada para gerar matrix de confunsão\n",
        "obs: é necessario fazer algumas modificações para gerar o gráfico, por isso irei deixar comentado"
      ]
    }
  ]
}